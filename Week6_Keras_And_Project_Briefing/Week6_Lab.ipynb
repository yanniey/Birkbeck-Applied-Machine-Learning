{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Keras + Tensorflow for Multi-layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the random number generator by setting the seed value\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model by using `Sequential`\n",
    "\n",
    "Models in Keras are defined as a sequence of layers. We create a `ğ‘†ğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘ğ‘™` model and add layers one at a time until we are happy with our network topology. The first thing to get right is to ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the input dim argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "\n",
    "**Fully connected layers** are defined using the `Dense` class. We can specify the number of neurons in the layer as the first argument and specify the activation function using the `ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›` argument. We will use the rectifier (ğ‘Ÿğ‘’ğ‘™ğ‘¢) activation function on the first two layers and the sigmoid activation function in the output layer. It used to be the case that ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ and ğ‘¡ğ‘ğ‘›h activation functions were preferred for all layers. These days, better performance is seen using the ğ‘Ÿğ‘’ğ‘™ğ‘¢ activation function. We use a ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ activation function on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5. We can piece it all together by adding each layer. The first hidden layer has 12 neurons and expects 8 input variables (e.g. ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ ğ‘‘ğ‘–ğ‘š = 8). The second hidden layer has 8 neurons and finally the output layer has 1 neuron to predict the class (onset of diabetes or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
