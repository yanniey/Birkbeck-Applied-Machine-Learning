{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Keras + Tensorflow for Multi-layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the random number generator by setting the seed value\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model with `Sequential()`\n",
    "\n",
    "Models in Keras are defined as a sequence of layers. We create a `𝑆𝑒𝑞𝑢𝑒𝑛𝑡𝑖𝑎𝑙` model and add layers one at a time until we are happy with our network topology. The first thing to get right is to ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the input dim argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "\n",
    "**Fully connected layers** are defined using the `Dense` class. We can specify the number of neurons in the layer as the first argument and specify the activation function using the `𝑎𝑐𝑡𝑖𝑣𝑎𝑡𝑖𝑜𝑛` argument. We will use the rectifier (𝑟𝑒𝑙𝑢) activation function on the first two layers and the sigmoid activation function in the output layer. It used to be the case that 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 and 𝑡𝑎𝑛h activation functions were preferred for all layers. These days, better performance is seen using the 𝑟𝑒𝑙𝑢 activation function. We use a 𝑠𝑖𝑔𝑚𝑜𝑖𝑑 activation function on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5. We can piece it all together by adding each layer. The first hidden layer has 12 neurons and expects 8 input variables (e.g. 𝑖𝑛𝑝𝑢𝑡 𝑑𝑖𝑚 = 8). The second hidden layer has 8 neurons and finally the output layer has 1 neuron to predict the class (onset of diabetes or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model with `compile()`\n",
    "\n",
    "We must specify the **loss function** to use to evaluate a set of weights, the **optimizer** used to search through different weights for the network and any optional metrics we would like to collect and report during training. In this case we will use 𝑙𝑜𝑔𝑎𝑟𝑖𝑡h𝑚𝑖𝑐 𝑙𝑜𝑠𝑠, which for a binary classification problem is defined in Keras as 𝑏𝑖𝑛𝑎𝑟𝑦_𝑐𝑟𝑜𝑠𝑠𝑒𝑛𝑡𝑟𝑜𝑝𝑦. We will also use the efficient gradient descent algorithm 𝑎𝑑𝑎𝑚 for no other reason that it is an efficient default\n",
    "\n",
    "* loss function = `binary_crossentropy`\n",
    "* optimizer = `adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model with `fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the epochs argument. We can also set the number of instances that are evaluated before a weight update in the network is performed called the batch size and set using the batch size argument. For this problem we will run for a small number of epochs (50) and use a relatively small batch size of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "768/768 [==============================] - 1s 720us/step - loss: 3.7093 - acc: 0.5977\n",
      "Epoch 2/50\n",
      "768/768 [==============================] - 0s 183us/step - loss: 0.9371 - acc: 0.5911\n",
      "Epoch 3/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.7477 - acc: 0.6432\n",
      "Epoch 4/50\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.7119 - acc: 0.6549\n",
      "Epoch 5/50\n",
      "768/768 [==============================] - 0s 173us/step - loss: 0.6841 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.6523 - acc: 0.6784\n",
      "Epoch 7/50\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.6505 - acc: 0.6810\n",
      "Epoch 8/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.6389 - acc: 0.6810\n",
      "Epoch 9/50\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.6258 - acc: 0.6953\n",
      "Epoch 10/50\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.6313 - acc: 0.6771\n",
      "Epoch 11/50\n",
      "768/768 [==============================] - 0s 171us/step - loss: 0.6505 - acc: 0.6719\n",
      "Epoch 12/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.6393 - acc: 0.6732\n",
      "Epoch 13/50\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.6254 - acc: 0.6745\n",
      "Epoch 14/50\n",
      "768/768 [==============================] - 0s 237us/step - loss: 0.6182 - acc: 0.6979\n",
      "Epoch 15/50\n",
      "768/768 [==============================] - 0s 202us/step - loss: 0.6019 - acc: 0.6953\n",
      "Epoch 16/50\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.5879 - acc: 0.6979\n",
      "Epoch 17/50\n",
      "768/768 [==============================] - 0s 190us/step - loss: 0.5849 - acc: 0.7005\n",
      "Epoch 18/50\n",
      "768/768 [==============================] - 0s 195us/step - loss: 0.6001 - acc: 0.6875\n",
      "Epoch 19/50\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.5798 - acc: 0.7096\n",
      "Epoch 20/50\n",
      "768/768 [==============================] - 0s 178us/step - loss: 0.5796 - acc: 0.7227\n",
      "Epoch 21/50\n",
      "768/768 [==============================] - 0s 187us/step - loss: 0.5684 - acc: 0.7174\n",
      "Epoch 22/50\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.5818 - acc: 0.6966\n",
      "Epoch 23/50\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5738 - acc: 0.7135\n",
      "Epoch 24/50\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.5677 - acc: 0.7318\n",
      "Epoch 25/50\n",
      "768/768 [==============================] - 0s 197us/step - loss: 0.5575 - acc: 0.7357\n",
      "Epoch 26/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.5710 - acc: 0.7044\n",
      "Epoch 27/50\n",
      "768/768 [==============================] - 0s 193us/step - loss: 0.5555 - acc: 0.7188\n",
      "Epoch 28/50\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.5552 - acc: 0.7305\n",
      "Epoch 29/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.5739 - acc: 0.7135\n",
      "Epoch 30/50\n",
      "768/768 [==============================] - 0s 319us/step - loss: 0.5612 - acc: 0.7214\n",
      "Epoch 31/50\n",
      "768/768 [==============================] - 0s 221us/step - loss: 0.5691 - acc: 0.7161\n",
      "Epoch 32/50\n",
      "768/768 [==============================] - 0s 214us/step - loss: 0.5641 - acc: 0.7135\n",
      "Epoch 33/50\n",
      "768/768 [==============================] - 0s 241us/step - loss: 0.5513 - acc: 0.7227\n",
      "Epoch 34/50\n",
      "768/768 [==============================] - 0s 232us/step - loss: 0.5508 - acc: 0.7305\n",
      "Epoch 35/50\n",
      "768/768 [==============================] - 0s 238us/step - loss: 0.5512 - acc: 0.7253\n",
      "Epoch 36/50\n",
      "768/768 [==============================] - 0s 189us/step - loss: 0.5598 - acc: 0.7057\n",
      "Epoch 37/50\n",
      "768/768 [==============================] - 0s 188us/step - loss: 0.5363 - acc: 0.7370\n",
      "Epoch 38/50\n",
      "768/768 [==============================] - 0s 182us/step - loss: 0.5409 - acc: 0.7227\n",
      "Epoch 39/50\n",
      "768/768 [==============================] - 0s 197us/step - loss: 0.5448 - acc: 0.7227\n",
      "Epoch 40/50\n",
      "768/768 [==============================] - 0s 209us/step - loss: 0.5435 - acc: 0.7240\n",
      "Epoch 41/50\n",
      "768/768 [==============================] - 0s 299us/step - loss: 0.5433 - acc: 0.7357\n",
      "Epoch 42/50\n",
      "768/768 [==============================] - 0s 198us/step - loss: 0.5356 - acc: 0.7331\n",
      "Epoch 43/50\n",
      "768/768 [==============================] - 0s 205us/step - loss: 0.5318 - acc: 0.7513\n",
      "Epoch 44/50\n",
      "768/768 [==============================] - 0s 185us/step - loss: 0.5328 - acc: 0.7409\n",
      "Epoch 45/50\n",
      "768/768 [==============================] - 0s 196us/step - loss: 0.5329 - acc: 0.7578\n",
      "Epoch 46/50\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.5281 - acc: 0.7513\n",
      "Epoch 47/50\n",
      "768/768 [==============================] - 0s 204us/step - loss: 0.5320 - acc: 0.7344\n",
      "Epoch 48/50\n",
      "768/768 [==============================] - 0s 202us/step - loss: 0.5335 - acc: 0.7409\n",
      "Epoch 49/50\n",
      "768/768 [==============================] - 0s 233us/step - loss: 0.5326 - acc: 0.7513\n",
      "Epoch 50/50\n",
      "768/768 [==============================] - 0s 248us/step - loss: 0.5281 - acc: 0.7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb317372e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, Y, epochs = 50, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model with `evaluate()`\n",
    "\n",
    "You can evaluate your model on your training dataset using the 𝑒𝑣𝑎𝑙𝑢𝑎𝑡𝑖𝑜𝑛() function on your model and pass it the same input and output used to train the model. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 88us/step\n",
      "\n",
      "acc: 76.04%\n"
     ]
    }
   ],
   "source": [
    "# evluate the model using the training dataset\n",
    "scores = model.evaluate(X,Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Keras provides two ways to split data:\n",
    "1. Automatic verification with `fit()`\n",
    "2. Manual verification with `train_test_split()`\n",
    "3. K-fold verification with `StratifiedKFold()`\n",
    "\n",
    "**Automatic verfication dataset** \n",
    "\n",
    "Keras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset each epoch. You can do this by setting the validation split argument on the 𝑓𝑖𝑡() function to a percentage of the size of your training dataset. For example, a reasonable value might be 0.2 or 0.33 for 20% or 33% of your training data held back for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 0s 932us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 234us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 217us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 259us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 234us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 206us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 223us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 220us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 201us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 239us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 244us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 244us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 265us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 257us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 274us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 201us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 251us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 215us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 247us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 340us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 248us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 250us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 236us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 251us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 212us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 202us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 200us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 257us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 198us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 167us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 193us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 202us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 192us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 215us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 206us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb31be7550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic verification dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X,Y,validation_split=0.33, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Verification Dataset** \n",
    "\n",
    "Keras also allows you to manually specify the dataset to use for validation during training. In this example we use the handy train test 𝑠𝑝𝑙𝑖𝑡() function from the Python scikit-learn machine learning library to separate our data into a training and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 1ms/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 248us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 226us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 286us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 235us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 246us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 209us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 349us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 253us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 263us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 279us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 227us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 402us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 391us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 249us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 266us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 220us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 269us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 256us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 260us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 224us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 280us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 240us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 261us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 278us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 224us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 217us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 207us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 218us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 323us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 385us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 270us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 240us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 230us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 316us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 291us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 291us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 207us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 193us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 195us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 221us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 212us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a332d3d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic verification dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33, random_state=7)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual k-Fold Cross-Validation**\n",
    "\n",
    "* **StratifiedKFold** means that the algorithm attemps to balance the number of instances of each class in each fold. \n",
    "* Verbose output for each epoch is turned off by using `verbose = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.94%\n",
      "acc: 67.53%\n",
      "acc: 62.34%\n",
      "acc: 77.92%\n",
      "acc: 76.62%\n",
      "acc: 72.73%\n",
      "acc: 63.64%\n",
      "acc: 59.74%\n",
      "acc: 71.05%\n",
      "acc: 61.84%\n",
      "67.83% (+/- 6.09%)\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold verification \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create a 10-fold cross validation \n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=7)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X,Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "    model.add(Dense(8,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "    model.fit(X[train],Y[train], epochs=50, batch_size=10,verbose=0)\n",
    "    scores = model.evaluate(X[test],Y[test],verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1],scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras with Scikit-Learn: `KerasClassifier` and `KerasRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `𝐾𝑒𝑟𝑎𝑠𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑒𝑟` and `𝐾𝑒𝑟𝑎𝑠𝑅𝑒𝑔𝑟𝑒𝑠𝑠𝑜𝑟` classes in Keras take an argument `𝑏𝑢𝑖𝑙𝑑_𝑓𝑛` which is the name of the function to call to create your model. You must define a function called whatever you like that defines your model, compiles it and returns it. In the example below we define a function `𝑐𝑟𝑒𝑎𝑡𝑒_𝑚𝑜𝑑𝑒𝑙()` that create a simple multilayer neural network for the problem.\n",
    "\n",
    "We pass this function name to the `𝐾𝑒𝑟𝑎𝑠𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑒𝑟` class by the `𝑏𝑢𝑖𝑙𝑑_𝑓𝑛` argument. We also pass in additional arguments of `𝑒𝑝𝑜𝑐h𝑠` = 150 and `𝑏𝑎𝑡𝑐h 𝑠𝑖𝑧𝑒` = 10. These are automatically bundled up and passed on to the `𝑓𝑖𝑡()` function which is called internally by the `𝐾𝑒𝑟𝑎𝑠𝐶𝑙𝑎𝑠𝑠𝑖𝑓𝑖𝑒𝑟` class. In this example we use the scikit-learn `𝑆𝑡𝑟𝑎𝑡𝑖𝑓𝑖𝑒𝑑𝐾𝐹𝑜𝑙𝑑` to perform 10-fold stratified cross- validation. This is a resampling technique that can provide a robust estimate of the performance of a machine learning model on unseen data. We use the scikit-learn function `𝑐𝑟𝑜𝑠𝑠_𝑣𝑎𝑙_𝑠𝑐𝑜𝑟𝑒()` to evaluate our model using the cross-validation scheme and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939337005088856\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# create a function to build a model, which is required by Kerasclassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "    model.add(Dense(8,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs = 150, batch_size=10,verbose=0)\n",
    "# 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state= 7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Deep Learning Model Parameters: `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best estimated performance. The `create_𝑚𝑜𝑑𝑒𝑙()` function is defined to take two arguments `𝑜𝑝𝑡𝑖𝑚𝑖𝑧𝑒𝑟` and `𝑖𝑛𝑖𝑡`, both of which must have default values. This will allow us to evaluate the effect of using different optimisation algorithms and weight initialisation schemes for our network. After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "\n",
    "* **Optimizers** for searching different weight values.\n",
    "* **Initializers** for preparing the network weights using different schemes.\n",
    "* **Number of epochs** for training the model for different number of exposures to the training dataset.\n",
    "* **Batches** for varying the number of samples before weight updates.\n",
    "\n",
    "The options are specified into a dictionary and passed to the configuration of the `𝐺𝑟𝑖𝑑𝑆𝑒𝑎𝑟𝑐h𝐶𝑉` scikit-learn class. This class will evaluate a version of our neural network model for each com- bination of parameters (2 × 3 × 3 × 3) for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-77d134cf441b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m-> 1470\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import numpy\n",
    "\n",
    "# create a function to build a model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu')) \n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7 \n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data & split into input (X) and output (Y) variables\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8] \n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal', 'uniform'] \n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params): \n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
