{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Keras + Tensorflow for Multi-layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise the random number generator by setting the seed value\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "np.random.seed(7)\n",
    "\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Model with `Sequential()`\n",
    "\n",
    "Models in Keras are defined as a sequence of layers. We create a `ğ‘†ğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘ğ‘™` model and add layers one at a time until we are happy with our network topology. The first thing to get right is to ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the input dim argument and setting it to 8 for the 8 input variables.\n",
    "\n",
    "\n",
    "**Fully connected layers** are defined using the `Dense` class. We can specify the number of neurons in the layer as the first argument and specify the activation function using the `ğ‘ğ‘ğ‘¡ğ‘–ğ‘£ğ‘ğ‘¡ğ‘–ğ‘œğ‘›` argument. We will use the rectifier (ğ‘Ÿğ‘’ğ‘™ğ‘¢) activation function on the first two layers and the sigmoid activation function in the output layer. It used to be the case that ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ and ğ‘¡ğ‘ğ‘›h activation functions were preferred for all layers. These days, better performance is seen using the ğ‘Ÿğ‘’ğ‘™ğ‘¢ activation function. We use a ğ‘ ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ activation function on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5. We can piece it all together by adding each layer. The first hidden layer has 12 neurons and expects 8 input variables (e.g. ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ ğ‘‘ğ‘–ğ‘š = 8). The second hidden layer has 8 neurons and finally the output layer has 1 neuron to predict the class (onset of diabetes or not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model with `compile()`\n",
    "\n",
    "We must specify the **loss function** to use to evaluate a set of weights, the **optimizer** used to search through different weights for the network and any optional metrics we would like to collect and report during training. In this case we will use ğ‘™ğ‘œğ‘”ğ‘ğ‘Ÿğ‘–ğ‘¡hğ‘šğ‘–ğ‘ ğ‘™ğ‘œğ‘ ğ‘ , which for a binary classification problem is defined in Keras as ğ‘ğ‘–ğ‘›ğ‘ğ‘Ÿğ‘¦_ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ ğ‘’ğ‘›ğ‘¡ğ‘Ÿğ‘œğ‘ğ‘¦. We will also use the efficient gradient descent algorithm ğ‘ğ‘‘ğ‘ğ‘š for no other reason that it is an efficient default\n",
    "\n",
    "* loss function = `binary_crossentropy`\n",
    "* optimizer = `adam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model with `fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process will run for a fixed number of iterations through the dataset called epochs, that we must specify using the epochs argument. We can also set the number of instances that are evaluated before a weight update in the network is performed called the batch size and set using the batch size argument. For this problem we will run for a small number of epochs (50) and use a relatively small batch size of 10. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "768/768 [==============================] - 1s 720us/step - loss: 3.7093 - acc: 0.5977\n",
      "Epoch 2/50\n",
      "768/768 [==============================] - 0s 183us/step - loss: 0.9371 - acc: 0.5911\n",
      "Epoch 3/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.7477 - acc: 0.6432\n",
      "Epoch 4/50\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.7119 - acc: 0.6549\n",
      "Epoch 5/50\n",
      "768/768 [==============================] - 0s 173us/step - loss: 0.6841 - acc: 0.6667\n",
      "Epoch 6/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.6523 - acc: 0.6784\n",
      "Epoch 7/50\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.6505 - acc: 0.6810\n",
      "Epoch 8/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.6389 - acc: 0.6810\n",
      "Epoch 9/50\n",
      "768/768 [==============================] - 0s 177us/step - loss: 0.6258 - acc: 0.6953\n",
      "Epoch 10/50\n",
      "768/768 [==============================] - 0s 167us/step - loss: 0.6313 - acc: 0.6771\n",
      "Epoch 11/50\n",
      "768/768 [==============================] - 0s 171us/step - loss: 0.6505 - acc: 0.6719\n",
      "Epoch 12/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.6393 - acc: 0.6732\n",
      "Epoch 13/50\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.6254 - acc: 0.6745\n",
      "Epoch 14/50\n",
      "768/768 [==============================] - 0s 237us/step - loss: 0.6182 - acc: 0.6979\n",
      "Epoch 15/50\n",
      "768/768 [==============================] - 0s 202us/step - loss: 0.6019 - acc: 0.6953\n",
      "Epoch 16/50\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.5879 - acc: 0.6979\n",
      "Epoch 17/50\n",
      "768/768 [==============================] - 0s 190us/step - loss: 0.5849 - acc: 0.7005\n",
      "Epoch 18/50\n",
      "768/768 [==============================] - 0s 195us/step - loss: 0.6001 - acc: 0.6875\n",
      "Epoch 19/50\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.5798 - acc: 0.7096\n",
      "Epoch 20/50\n",
      "768/768 [==============================] - 0s 178us/step - loss: 0.5796 - acc: 0.7227\n",
      "Epoch 21/50\n",
      "768/768 [==============================] - 0s 187us/step - loss: 0.5684 - acc: 0.7174\n",
      "Epoch 22/50\n",
      "768/768 [==============================] - 0s 192us/step - loss: 0.5818 - acc: 0.6966\n",
      "Epoch 23/50\n",
      "768/768 [==============================] - 0s 172us/step - loss: 0.5738 - acc: 0.7135\n",
      "Epoch 24/50\n",
      "768/768 [==============================] - 0s 180us/step - loss: 0.5677 - acc: 0.7318\n",
      "Epoch 25/50\n",
      "768/768 [==============================] - 0s 197us/step - loss: 0.5575 - acc: 0.7357\n",
      "Epoch 26/50\n",
      "768/768 [==============================] - 0s 165us/step - loss: 0.5710 - acc: 0.7044\n",
      "Epoch 27/50\n",
      "768/768 [==============================] - 0s 193us/step - loss: 0.5555 - acc: 0.7188\n",
      "Epoch 28/50\n",
      "768/768 [==============================] - 0s 176us/step - loss: 0.5552 - acc: 0.7305\n",
      "Epoch 29/50\n",
      "768/768 [==============================] - 0s 175us/step - loss: 0.5739 - acc: 0.7135\n",
      "Epoch 30/50\n",
      "768/768 [==============================] - 0s 319us/step - loss: 0.5612 - acc: 0.7214\n",
      "Epoch 31/50\n",
      "768/768 [==============================] - 0s 221us/step - loss: 0.5691 - acc: 0.7161\n",
      "Epoch 32/50\n",
      "768/768 [==============================] - 0s 214us/step - loss: 0.5641 - acc: 0.7135\n",
      "Epoch 33/50\n",
      "768/768 [==============================] - 0s 241us/step - loss: 0.5513 - acc: 0.7227\n",
      "Epoch 34/50\n",
      "768/768 [==============================] - 0s 232us/step - loss: 0.5508 - acc: 0.7305\n",
      "Epoch 35/50\n",
      "768/768 [==============================] - 0s 238us/step - loss: 0.5512 - acc: 0.7253\n",
      "Epoch 36/50\n",
      "768/768 [==============================] - 0s 189us/step - loss: 0.5598 - acc: 0.7057\n",
      "Epoch 37/50\n",
      "768/768 [==============================] - 0s 188us/step - loss: 0.5363 - acc: 0.7370\n",
      "Epoch 38/50\n",
      "768/768 [==============================] - 0s 182us/step - loss: 0.5409 - acc: 0.7227\n",
      "Epoch 39/50\n",
      "768/768 [==============================] - 0s 197us/step - loss: 0.5448 - acc: 0.7227\n",
      "Epoch 40/50\n",
      "768/768 [==============================] - 0s 209us/step - loss: 0.5435 - acc: 0.7240\n",
      "Epoch 41/50\n",
      "768/768 [==============================] - 0s 299us/step - loss: 0.5433 - acc: 0.7357\n",
      "Epoch 42/50\n",
      "768/768 [==============================] - 0s 198us/step - loss: 0.5356 - acc: 0.7331\n",
      "Epoch 43/50\n",
      "768/768 [==============================] - 0s 205us/step - loss: 0.5318 - acc: 0.7513\n",
      "Epoch 44/50\n",
      "768/768 [==============================] - 0s 185us/step - loss: 0.5328 - acc: 0.7409\n",
      "Epoch 45/50\n",
      "768/768 [==============================] - 0s 196us/step - loss: 0.5329 - acc: 0.7578\n",
      "Epoch 46/50\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.5281 - acc: 0.7513\n",
      "Epoch 47/50\n",
      "768/768 [==============================] - 0s 204us/step - loss: 0.5320 - acc: 0.7344\n",
      "Epoch 48/50\n",
      "768/768 [==============================] - 0s 202us/step - loss: 0.5335 - acc: 0.7409\n",
      "Epoch 49/50\n",
      "768/768 [==============================] - 0s 233us/step - loss: 0.5326 - acc: 0.7513\n",
      "Epoch 50/50\n",
      "768/768 [==============================] - 0s 248us/step - loss: 0.5281 - acc: 0.7357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb317372e8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, Y, epochs = 50, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model with `evaluate()`\n",
    "\n",
    "You can evaluate your model on your training dataset using the ğ‘’ğ‘£ğ‘ğ‘™ğ‘¢ğ‘ğ‘¡ğ‘–ğ‘œğ‘›() function on your model and pass it the same input and output used to train the model. This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 88us/step\n",
      "\n",
      "acc: 76.04%\n"
     ]
    }
   ],
   "source": [
    "# evluate the model using the training dataset\n",
    "scores = model.evaluate(X,Y)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1],scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Splitting\n",
    "\n",
    "Keras provides two ways to split data:\n",
    "1. Automatic verification with `fit()`\n",
    "2. Manual verification with `train_test_split()`\n",
    "3. K-fold verification with `StratifiedKFold()`\n",
    "\n",
    "**Automatic verfication dataset** \n",
    "\n",
    "Keras can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset each epoch. You can do this by setting the validation split argument on the ğ‘“ğ‘–ğ‘¡() function to a percentage of the size of your training dataset. For example, a reasonable value might be 0.2 or 0.33 for 20% or 33% of your training data held back for validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 0s 932us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 234us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 217us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 259us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 234us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 206us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 223us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 220us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 201us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 239us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 244us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 244us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 265us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 257us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 274us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 201us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 251us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 215us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 247us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 340us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 248us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 250us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 236us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 251us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 212us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 202us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 200us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 205us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 257us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 198us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 194us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 208us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 184us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 167us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 193us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 202us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 192us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 215us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 206us/step - loss: 5.8013 - acc: 0.6401 - val_loss: 5.2669 - val_acc: 0.6732\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb31be7550>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic verification dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X,Y,validation_split=0.33, epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual Verification Dataset** \n",
    "\n",
    "Keras also allows you to manually specify the dataset to use for validation during training. In this example we use the handy train test ğ‘ ğ‘ğ‘™ğ‘–ğ‘¡() function from the Python scikit-learn machine learning library to separate our data into a training and test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 514 samples, validate on 254 samples\n",
      "Epoch 1/50\n",
      "514/514 [==============================] - 1s 1ms/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 2/50\n",
      "514/514 [==============================] - 0s 248us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 3/50\n",
      "514/514 [==============================] - 0s 226us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 4/50\n",
      "514/514 [==============================] - 0s 286us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 5/50\n",
      "514/514 [==============================] - 0s 235us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 6/50\n",
      "514/514 [==============================] - 0s 246us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 7/50\n",
      "514/514 [==============================] - 0s 209us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 8/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 9/50\n",
      "514/514 [==============================] - 0s 349us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 10/50\n",
      "514/514 [==============================] - 0s 253us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 11/50\n",
      "514/514 [==============================] - 0s 263us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 12/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 13/50\n",
      "514/514 [==============================] - 0s 245us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 14/50\n",
      "514/514 [==============================] - 0s 279us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 15/50\n",
      "514/514 [==============================] - 0s 227us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 16/50\n",
      "514/514 [==============================] - 0s 402us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 17/50\n",
      "514/514 [==============================] - 0s 391us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 18/50\n",
      "514/514 [==============================] - 0s 249us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 19/50\n",
      "514/514 [==============================] - 0s 266us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 20/50\n",
      "514/514 [==============================] - 0s 220us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 21/50\n",
      "514/514 [==============================] - 0s 269us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 22/50\n",
      "514/514 [==============================] - 0s 256us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 23/50\n",
      "514/514 [==============================] - 0s 260us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 24/50\n",
      "514/514 [==============================] - 0s 224us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 25/50\n",
      "514/514 [==============================] - 0s 280us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 26/50\n",
      "514/514 [==============================] - 0s 240us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 27/50\n",
      "514/514 [==============================] - 0s 261us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 28/50\n",
      "514/514 [==============================] - 0s 278us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 29/50\n",
      "514/514 [==============================] - 0s 224us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 30/50\n",
      "514/514 [==============================] - 0s 217us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 31/50\n",
      "514/514 [==============================] - 0s 207us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 32/50\n",
      "514/514 [==============================] - 0s 218us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 33/50\n",
      "514/514 [==============================] - 0s 323us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 34/50\n",
      "514/514 [==============================] - 0s 385us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 35/50\n",
      "514/514 [==============================] - 0s 270us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 36/50\n",
      "514/514 [==============================] - 0s 240us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 37/50\n",
      "514/514 [==============================] - 0s 230us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 38/50\n",
      "514/514 [==============================] - 0s 316us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 39/50\n",
      "514/514 [==============================] - 0s 291us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 40/50\n",
      "514/514 [==============================] - 0s 291us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 41/50\n",
      "514/514 [==============================] - 0s 207us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 42/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 43/50\n",
      "514/514 [==============================] - 0s 189us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 44/50\n",
      "514/514 [==============================] - 0s 242us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 45/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 46/50\n",
      "514/514 [==============================] - 0s 210us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 47/50\n",
      "514/514 [==============================] - 0s 193us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 48/50\n",
      "514/514 [==============================] - 0s 195us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 49/50\n",
      "514/514 [==============================] - 0s 221us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n",
      "Epoch 50/50\n",
      "514/514 [==============================] - 0s 212us/step - loss: 5.5190 - acc: 0.6576 - val_loss: 5.8381 - val_acc: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a332d3d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# automatic verification dataset\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.33, random_state=7)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "model.add(Dense(8,activation=\"relu\"))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test), epochs=50, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Manual k-Fold Cross-Validation**\n",
    "\n",
    "* **StratifiedKFold** means that the algorithm attemps to balance the number of instances of each class in each fold. \n",
    "* Verbose output for each epoch is turned off by using `verbose = 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 64.94%\n",
      "acc: 67.53%\n",
      "acc: 62.34%\n",
      "acc: 77.92%\n",
      "acc: 76.62%\n",
      "acc: 72.73%\n",
      "acc: 63.64%\n",
      "acc: 59.74%\n",
      "acc: 71.05%\n",
      "acc: 61.84%\n",
      "67.83% (+/- 6.09%)\n"
     ]
    }
   ],
   "source": [
    "# StratifiedKFold verification \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(7)\n",
    "dataset = np.loadtxt('pima-indians-diabetes.data.csv',delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create a 10-fold cross validation \n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True,random_state=7)\n",
    "cvscores = []\n",
    "\n",
    "for train, test in kfold.split(X,Y):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "    model.add(Dense(8,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "    model.fit(X[train],Y[train], epochs=50, batch_size=10,verbose=0)\n",
    "    scores = model.evaluate(X[test],Y[test],verbose=0)\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1],scores[1]*100))\n",
    "    cvscores.append(scores[1]*100)\n",
    "\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Keras with Scikit-Learn: `KerasClassifier` and `KerasRegressor`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ğ¾ğ‘’ğ‘Ÿğ‘ğ‘ ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ` and `ğ¾ğ‘’ğ‘Ÿğ‘ğ‘ ğ‘…ğ‘’ğ‘”ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘œğ‘Ÿ` classes in Keras take an argument `ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘_ğ‘“ğ‘›` which is the name of the function to call to create your model. You must define a function called whatever you like that defines your model, compiles it and returns it. In the example below we define a function `ğ‘ğ‘Ÿğ‘’ğ‘ğ‘¡ğ‘’_ğ‘šğ‘œğ‘‘ğ‘’ğ‘™()` that create a simple multilayer neural network for the problem.\n",
    "\n",
    "We pass this function name to the `ğ¾ğ‘’ğ‘Ÿğ‘ğ‘ ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ` class by the `ğ‘ğ‘¢ğ‘–ğ‘™ğ‘‘_ğ‘“ğ‘›` argument. We also pass in additional arguments of `ğ‘’ğ‘ğ‘œğ‘hğ‘ ` = 150 and `ğ‘ğ‘ğ‘¡ğ‘h ğ‘ ğ‘–ğ‘§ğ‘’` = 10. These are automatically bundled up and passed on to the `ğ‘“ğ‘–ğ‘¡()` function which is called internally by the `ğ¾ğ‘’ğ‘Ÿğ‘ğ‘ ğ¶ğ‘™ğ‘ğ‘ ğ‘ ğ‘–ğ‘“ğ‘–ğ‘’ğ‘Ÿ` class. In this example we use the scikit-learn `ğ‘†ğ‘¡ğ‘Ÿğ‘ğ‘¡ğ‘–ğ‘“ğ‘–ğ‘’ğ‘‘ğ¾ğ¹ğ‘œğ‘™ğ‘‘` to perform 10-fold stratified cross- validation. This is a resampling technique that can provide a robust estimate of the performance of a machine learning model on unseen data. We use the scikit-learn function `ğ‘ğ‘Ÿğ‘œğ‘ ğ‘ _ğ‘£ğ‘ğ‘™_ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’()` to evaluate our model using the cross-validation scheme and print the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6939337005088856\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# create a function to build a model, which is required by Kerasclassifier\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation=\"relu\"))\n",
    "    model.add(Dense(8,activation=\"relu\"))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, epochs = 150, batch_size=10,verbose=0)\n",
    "# 10-fold cross validation\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state= 7)\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "print(results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Deep Learning Model Parameters: `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best estimated performance. The `create_ğ‘šğ‘œğ‘‘ğ‘’ğ‘™()` function is defined to take two arguments `ğ‘œğ‘ğ‘¡ğ‘–ğ‘šğ‘–ğ‘§ğ‘’ğ‘Ÿ` and `ğ‘–ğ‘›ğ‘–ğ‘¡`, both of which must have default values. This will allow us to evaluate the effect of using different optimisation algorithms and weight initialisation schemes for our network. After creating our model, we define arrays of values for the parameter we wish to search, specifically:\n",
    "\n",
    "* **Optimizers** for searching different weight values.\n",
    "* **Initializers** for preparing the network weights using different schemes.\n",
    "* **Number of epochs** for training the model for different number of exposures to the training dataset.\n",
    "* **Batches** for varying the number of samples before weight updates.\n",
    "\n",
    "The options are specified into a dictionary and passed to the configuration of the `ğºğ‘Ÿğ‘–ğ‘‘ğ‘†ğ‘’ğ‘ğ‘Ÿğ‘hğ¶ğ‘‰` scikit-learn class. This class will evaluate a version of our neural network model for each com- bination of parameters (2 Ã— 3 Ã— 3 Ã— 3) for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-77d134cf441b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mgrid_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# summarize results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    638\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    639\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 640\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m         \u001b[0mtest_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_score\u001b[0;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \"\"\"\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[0;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 553\u001b[0;31m             \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/sklearn/metrics/scorer.py\u001b[0m in \u001b[0;36m_passthrough_scorer\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_passthrough_scorer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;34m\"\"\"Function that wraps estimator.score\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps)\u001b[0m\n\u001b[1;32m   1111\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m                                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1113\u001b[0;31m                                          steps=steps)\n\u001b[0m\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m     def predict(self, x,\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2669\u001b[0m                                 \u001b[0mfeed_symbols\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m                                 session)\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_make_callable\u001b[0;34m(self, feed_arrays, feed_symbols, symbol_vals, session)\u001b[0m\n\u001b[1;32m   2621\u001b[0m             \u001b[0mcallable_opts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2622\u001b[0m         \u001b[0;31m# Create callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2623\u001b[0;31m         \u001b[0mcallable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_callable_from_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallable_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2624\u001b[0m         \u001b[0;31m# Cache parameters corresponding to the generated callable, so that\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2625\u001b[0m         \u001b[0;31m# we can detect future mismatches and refresh the callable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_make_callable_from_options\u001b[0;34m(self, callable_options)\u001b[0m\n\u001b[1;32m   1468\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1469\u001b[0m     \"\"\"\n\u001b[0;32m-> 1470\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1471\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mBaseSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallable_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from sklearn.model_selection import GridSearchCV \n",
    "import numpy\n",
    "\n",
    "# create a function to build a model, required for KerasClassifier\n",
    "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=8, kernel_initializer=init, activation='relu')) \n",
    "    model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
    "    model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
    "    # compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']) \n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7 \n",
    "numpy.random.seed(seed)\n",
    "\n",
    "# load data & split into input (X) and output (Y) variables\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.data.csv\", delimiter=\",\")\n",
    "X = dataset[:,0:8] \n",
    "Y = dataset[:,8]\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "\n",
    "# grid search epochs, batch size and optimizer\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "inits = ['glorot_uniform', 'normal', 'uniform'] \n",
    "epochs = [50, 100, 150]\n",
    "batches = [5, 10, 20]\n",
    "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=inits)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
    "grid_result = grid.fit(X, Y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params): \n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
