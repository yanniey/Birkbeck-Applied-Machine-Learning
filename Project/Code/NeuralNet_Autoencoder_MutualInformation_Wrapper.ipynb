{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoder and Classsification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ConMatAndStats.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "path = '../Data/'\n",
    "filename = 'selected_data.csv' \n",
    "testfilename = 'selected_test_data.csv'\n",
    "data = read_csv(path + filename)\n",
    "test = read_csv(path + testfilename)\n",
    "\n",
    "allnames = data.columns\n",
    "Xnames = data.columns[0:data.values.shape[1]-1]\n",
    "\n",
    "array = data.values \n",
    "X = array[:,0:array.shape[1]-1] \n",
    "Y = array[:,array.shape[1]-1]\n",
    "\n",
    "testarray = test.values \n",
    "Xtest = testarray[:,0:array.shape[1]-1] \n",
    "Ytest = testarray[:,array.shape[1]-1]\n",
    "\n",
    "Xsize = X.shape[1]\n",
    "\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA\n",
    "\n",
    "Rerun the PCA on the selected feature to get idea of how many indeoendent vectors there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "u, s, vh = numpy.linalg.svd(data.values, full_matrices=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 5 vectors explain 72.55% of all the variance in the dataset\n",
      "The first 10 vectors explain 89.50% of all the variance in the dataset\n",
      "The first 15 vectors explain 97.17% of all the variance in the dataset\n",
      "The first 20 vectors explain 99.84% of all the variance in the dataset\n",
      "The first 25 vectors explain 100.00% of all the variance in the dataset\n"
     ]
    }
   ],
   "source": [
    "for i in range (5, 30, 5):\n",
    "    print('The first %d vectors explain %5.2f%% of all the variance in the dataset' %(i, 100 * s[0:i].sum() / s.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy\n",
    "# fix random seed for reproducibility\n",
    "random_seed = 7\n",
    "numpy.random.seed(random_seed)\n",
    "\n",
    "NNLayerSize = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.33, random_state=random_seed, stratify=Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65019 samples, validate on 32025 samples\n",
      "Epoch 1/10\n",
      "65019/65019 [==============================] - 1s 12us/sample - loss: 0.3444 - accuracy: 0.8913 - val_loss: 0.1492 - val_accuracy: 0.9391\n",
      "Epoch 2/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1131 - accuracy: 0.9478 - val_loss: 0.0908 - val_accuracy: 0.9632\n",
      "Epoch 3/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0747 - accuracy: 0.9737 - val_loss: 0.0667 - val_accuracy: 0.9775\n",
      "Epoch 4/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0573 - accuracy: 0.9806 - val_loss: 0.0564 - val_accuracy: 0.9795\n",
      "Epoch 5/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0493 - accuracy: 0.9829 - val_loss: 0.0505 - val_accuracy: 0.9824\n",
      "Epoch 6/10\n",
      "65019/65019 [==============================] - 0s 3us/sample - loss: 0.0445 - accuracy: 0.9848 - val_loss: 0.0465 - val_accuracy: 0.9840\n",
      "Epoch 7/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0411 - accuracy: 0.9858 - val_loss: 0.0434 - val_accuracy: 0.9853\n",
      "Epoch 8/10\n",
      "65019/65019 [==============================] - 0s 3us/sample - loss: 0.0387 - accuracy: 0.9868 - val_loss: 0.0410 - val_accuracy: 0.9869\n",
      "Epoch 9/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0365 - accuracy: 0.9879 - val_loss: 0.0389 - val_accuracy: 0.9873\n",
      "Epoch 10/10\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0342 - accuracy: 0.9890 - val_loss: 0.0375 - val_accuracy: 0.9886\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0b80174a8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.set_seed(random_seed)\n",
    "BasicNNmodel = Sequential()\n",
    "BasicNNmodel.add(Dense(Xsize, input_dim=Xsize, activation='relu')) \n",
    "BasicNNmodel.add(Dense(NNLayerSize, activation='relu')) \n",
    "BasicNNmodel.add(Dense(1, activation='sigmoid'))\n",
    "BasicNNmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "BasicNNmodel.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[19651   428]\n",
      " [ 5181 14898]]\n",
      "Accuracy: 0.860  F1: 0.875  MCC: 0.742\n"
     ]
    }
   ],
   "source": [
    "Ypred = BasicNNmodel.predict_classes(Xtest)\n",
    "a = ConMatAndStats (Ytest, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple NN has reasonable predictive ability\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "# The Stacked Autoencoder\n",
    "\n",
    "First import required modules and split the training set into training and validation components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Stacked Autoencoder mdel\n",
    "\n",
    "Input layer takes the dimension of number of features read from file these are fed to a dense layer with number of neurons given by the Encoder_layer1_size variable\n",
    "\n",
    "The stacked layer is a second dense layer of size Encoder_stack_size ad this will be used as the output from the autoencoder.\n",
    "\n",
    "A final decoder layer is required for training, this has size the same as the input features and a sigmoid activation.\n",
    "\n",
    "The whole model will be trained by using the same X matrix as input and classification, as the aim of the model is to reduce the dimensions of the input to the number of neurons in the stacked layer, but still be able to retrieve as much of the input information as possible when this is decoded. The model is effectively a lossy compression and the trainin is to minimise the loss in the compression algorithm.\n",
    "\n",
    "Setting the model up with the separate layers and explicit connections rather than using Keras Sequential mechanism allows the intermediate layers to be called afterwards by defining separate models that use appropriate ayers as their input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "Encoder_layer1_size = 15\n",
    "Encoder_stack_size = 8\n",
    "\n",
    "input_data = Input(shape=(Xsize,))\n",
    "first_encoded = Dense(Encoder_layer1_size, activation='relu')(input_data)\n",
    "stacked_encoded = Dense(Encoder_stack_size, activation='relu')(first_encoded)\n",
    "decoded = Dense(Xsize, activation = 'sigmoid')(stacked_encoded)\n",
    "stacked_autoencoder = Model(input_data, decoded)\n",
    "\n",
    "decoder_layer = stacked_autoencoder.layers[-1]\n",
    "stacked_encoded_input = Input(shape=(Encoder_stack_size,))\n",
    "decoder = Model(stacked_encoded_input, decoder_layer(stacked_encoded_input))\n",
    "stacked_encoder = Model(input_data, stacked_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 65019 samples, validate on 32025 samples\n",
      "Epoch 1/25\n",
      "65019/65019 [==============================] - 1s 9us/sample - loss: 0.5595 - accuracy: 0.5645 - val_loss: 0.3087 - val_accuracy: 0.6909\n",
      "Epoch 2/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.2042 - accuracy: 0.7069 - val_loss: 0.1536 - val_accuracy: 0.7153\n",
      "Epoch 3/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1395 - accuracy: 0.7173 - val_loss: 0.1304 - val_accuracy: 0.7195\n",
      "Epoch 4/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1260 - accuracy: 0.7187 - val_loss: 0.1216 - val_accuracy: 0.7196\n",
      "Epoch 5/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1184 - accuracy: 0.7193 - val_loss: 0.1148 - val_accuracy: 0.7216\n",
      "Epoch 6/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1122 - accuracy: 0.7213 - val_loss: 0.1093 - val_accuracy: 0.7224\n",
      "Epoch 7/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1075 - accuracy: 0.7217 - val_loss: 0.1055 - val_accuracy: 0.7227\n",
      "Epoch 8/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1045 - accuracy: 0.7220 - val_loss: 0.1032 - val_accuracy: 0.7229\n",
      "Epoch 9/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1026 - accuracy: 0.7221 - val_loss: 0.1017 - val_accuracy: 0.7230\n",
      "Epoch 10/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1013 - accuracy: 0.7223 - val_loss: 0.1005 - val_accuracy: 0.7231\n",
      "Epoch 11/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.1003 - accuracy: 0.7223 - val_loss: 0.0996 - val_accuracy: 0.7231\n",
      "Epoch 12/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0995 - accuracy: 0.7224 - val_loss: 0.0989 - val_accuracy: 0.7231\n",
      "Epoch 13/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0988 - accuracy: 0.7224 - val_loss: 0.0983 - val_accuracy: 0.7232\n",
      "Epoch 14/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0983 - accuracy: 0.7224 - val_loss: 0.0979 - val_accuracy: 0.7232\n",
      "Epoch 15/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0979 - accuracy: 0.7224 - val_loss: 0.0975 - val_accuracy: 0.7232\n",
      "Epoch 16/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0976 - accuracy: 0.7224 - val_loss: 0.0972 - val_accuracy: 0.7232\n",
      "Epoch 17/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0973 - accuracy: 0.7224 - val_loss: 0.0969 - val_accuracy: 0.7232\n",
      "Epoch 18/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0970 - accuracy: 0.7224 - val_loss: 0.0966 - val_accuracy: 0.7232\n",
      "Epoch 19/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0967 - accuracy: 0.7224 - val_loss: 0.0964 - val_accuracy: 0.7232\n",
      "Epoch 20/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0965 - accuracy: 0.7224 - val_loss: 0.0961 - val_accuracy: 0.7232\n",
      "Epoch 21/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0963 - accuracy: 0.7224 - val_loss: 0.0959 - val_accuracy: 0.7232\n",
      "Epoch 22/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0960 - accuracy: 0.7224 - val_loss: 0.0957 - val_accuracy: 0.7232\n",
      "Epoch 23/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0958 - accuracy: 0.7224 - val_loss: 0.0954 - val_accuracy: 0.7233\n",
      "Epoch 24/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0955 - accuracy: 0.7224 - val_loss: 0.0952 - val_accuracy: 0.7233\n",
      "Epoch 25/25\n",
      "65019/65019 [==============================] - 0s 4us/sample - loss: 0.0953 - accuracy: 0.7225 - val_loss: 0.0949 - val_accuracy: 0.7233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0104810b8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_autoencoder.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "stacked_autoencoder.fit(X_train, X_train, validation_data=(X_val, X_val), epochs=25, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encode the datasets\n",
    "\n",
    "Having trained the encoder it is now possible to use the stacked_encoder to compress the three X datasets down to the size defined for the second dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "EncodeX = stacked_encoder.predict(X)\n",
    "Encode_X_train = stacked_encoder.predict(X_train)\n",
    "Encode_X_val = stacked_encoder.predict(X_val)\n",
    "Encode_Xtest = stacked_encoder.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Models\n",
    "\n",
    "Having seen from prvious runs that the decison tree and logistic regression gave the best results, these two models will now be run against the output from the stacked autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART\n",
      "Check against Validation Set ...\n",
      "Confusion Matrix\n",
      "[[15973    39]\n",
      " [   27 15986]]\n",
      "Accuracy: 0.998  F1: 0.998  MCC: 0.996\n",
      "Test check ...\n",
      "Confusion Matrix\n",
      "[[19821   258]\n",
      " [18751  1328]]\n",
      "Accuracy: 0.527  F1: 0.676  MCC: 0.137\n",
      "\n",
      "LR\n",
      "Check against Validation Set ...\n",
      "Confusion Matrix\n",
      "[[15562   450]\n",
      " [  246 15767]]\n",
      "Accuracy: 0.978  F1: 0.978  MCC: 0.957\n",
      "Test check ...\n",
      "Confusion Matrix\n",
      "[[19396   683]\n",
      " [  176 19903]]\n",
      "Accuracy: 0.979  F1: 0.978  MCC: 0.958\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv \n",
    "from matplotlib import pyplot \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "# prepare models \n",
    "models = []  \n",
    "models.append(('CART', DecisionTreeClassifier())) \n",
    "models.append(('LR', LogisticRegression(solver='liblinear'))) \n",
    "\n",
    "\n",
    "results = [] \n",
    "names = [] \n",
    "stats =[]\n",
    "scoring = 'accuracy' \n",
    "for name, model in models:   \n",
    "    print(name)\n",
    "    print(\"Check against Validation Set ...\")\n",
    "    model.fit(Encode_X_train, Y_train)\n",
    "    Ypred  = model.predict(Encode_X_val)\n",
    "    a = ConMatAndStats(Y_val, Ypred)\n",
    "    print(\"Test check ...\")\n",
    "    Ypred = model.predict(Encode_Xtest)\n",
    "    stats.append(ConMatAndStats (Ytest, Ypred))\n",
    "    \n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "The Logistic Regression model maintains better accuracy against the Test dataset and so would be the best available result so far, fir future tests only include LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []  \n",
    "models.append(('LR', LogisticRegression(solver='liblinear'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual Information Theoretic Selection\n",
    "\n",
    "Concatenate the encoded X matrices onto the train and test sets.\n",
    "\n",
    "For the expanded training set determine the features with highest mutual information against the test class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XplusEncode = numpy.concatenate((X, EncodeX), axis=1)\n",
    "XtestplusEncode = numpy.concatenate((Xtest, Encode_Xtest), axis=1)\n",
    "XtrainplusEncode = numpy.concatenate((X_train, Encode_X_train), axis=1)\n",
    "XvalplusEncode = numpy.concatenate((X_val, Encode_X_val), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "BestFits = mutual_info_classif(XplusEncode,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select\n",
    "\n",
    "And select the top features to be retained for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_selected = 5\n",
    "\n",
    "TopX = XplusEncode[:,BestFits.argsort()[-Features_selected:]]\n",
    "TopXtest = XtestplusEncode[:,BestFits.argsort()[-Features_selected:]]\n",
    "TopXtrain = XtrainplusEncode[:,BestFits.argsort()[-Features_selected:]]\n",
    "TopXval = XvalplusEncode[:,BestFits.argsort()[-Features_selected:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retest Models\n",
    "\n",
    "Then retest the decision tree and logistic regression models agains these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] \n",
    "names = [] \n",
    "stats =[]\n",
    "scoring = 'accuracy' \n",
    "for name, model in models:   \n",
    "    print(name)\n",
    "    print(\"Check against Validation Set ...\")\n",
    "    model.fit(TopXtrain, Y_train)\n",
    "    Ypred  = model.predict(TopXval)\n",
    "    a = ConMatAndStats(Y_val, Ypred)\n",
    "    print(\"Test check ...\")\n",
    "    Ypred = model.predict(TopXtest)\n",
    "    stats.append(ConMatAndStats (Ytest, Ypred))\n",
    "    \n",
    "    print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Results?\n",
    "\n",
    "results vary on runs but are similar to just using the output from the stacked autoencoder\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    ".\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Wrapper\n",
    "\n",
    "Now try with a Decison Tree Wrapper. Select larger group of top features from the previous Mutual Information ranking, then pass these to the decion tree wrapper t select the same number of features as previously used in the models. See if this gives a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to install package for tree wrapper\n",
    "# !pip install mlxtend "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "More_selected = 30\n",
    "\n",
    "MoreX = XplusEncode[:,BestFits.argsort()[-More_selected:]]\n",
    "MoreXtest = XtestplusEncode[:,BestFits.argsort()[-More_selected:]]\n",
    "MoreXtrain = XtrainplusEncode[:,BestFits.argsort()[-More_selected:]]\n",
    "MoreXval = XvalplusEncode[:,BestFits.argsort()[-More_selected:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selector = SequentialFeatureSelector(DecisionTreeClassifier(),\n",
    "           k_features=8, #Features_selected,\n",
    "           forward=True,\n",
    "           verbose=1,\n",
    "           scoring='roc_auc',\n",
    "           cv=4,\n",
    "           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = feature_selector.fit(MoreX, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopTreeX = MoreX[:,features.k_feature_idx_]\n",
    "TopTreeXtest = MoreXtest[:,features.k_feature_idx_]\n",
    "TopTreeXtrain = MoreXtrain[:,features.k_feature_idx_]\n",
    "TopTreeXval = MoreXval[:,features.k_feature_idx_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [] \n",
    "names = [] \n",
    "stats =[]\n",
    "scoring = 'accuracy' \n",
    "for name, model in models:   \n",
    "    print(name)\n",
    "    print(\"Check against Validation Set ...\")\n",
    "    model.fit(TopTreeXtrain, Y_train)\n",
    "    Ypred  = model.predict(TopTreeXval)\n",
    "    a = ConMatAndStats(Y_val, Ypred)\n",
    "    print(\"Test check ...\")\n",
    "    Ypred = model.predict(TopTreeXtest)\n",
    "    stats.append(ConMatAndStats (Ytest, Ypred))\n",
    "    \n",
    "    print()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More Neural Nets \n",
    "\n",
    "Basd on the outputs from the selections above\n",
    "\n",
    "\n",
    "# 1) NN using All X features, plus autoencoder output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "DataAndAutoencoderModel = Sequential()\n",
    "DataAndAutoencoderModel.add(Dense(XtrainplusEncode.shape[1], input_dim=XtrainplusEncode.shape[1], activation='relu')) \n",
    "DataAndAutoencoderModel.add(Dense(NNLayerSize, activation='relu')) \n",
    "DataAndAutoencoderModel.add(Dense(1, activation='sigmoid'))\n",
    "DataAndAutoencoderModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "DataAndAutoencoderModel.fit(XtrainplusEncode, Y_train, validation_data=(XvalplusEncode, Y_val), epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = DataAndAutoencoderModel.predict_classes(XtestplusEncode)\n",
    "a = ConMatAndStats (Ytest, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) NN using output from the MI filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "MIFilteredModel = Sequential()\n",
    "MIFilteredModel.add(Dense(TopXtrain.shape[1], input_dim=TopXtrain.shape[1], activation='relu')) \n",
    "MIFilteredModel.add(Dense(NNLayerSize, activation='relu')) \n",
    "MIFilteredModel.add(Dense(1, activation='sigmoid'))\n",
    "MIFilteredModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "MIFilteredModel.fit(TopXtrain, Y_train, validation_data=(TopXval, Y_val), epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = MIFilteredModel.predict_classes(TopXtest)\n",
    "a = ConMatAndStats (Ytest, Ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) NN using Output from the Tree Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "MIFilteredPLusTreeWrapperModel = Sequential()\n",
    "MIFilteredPLusTreeWrapperModel.add(Dense(TopTreeXtrain.shape[1], input_dim=TopTreeXtrain.shape[1], activation='relu')) \n",
    "MIFilteredPLusTreeWrapperModel.add(Dense(NNLayerSize, activation='relu')) \n",
    "MIFilteredPLusTreeWrapperModel.add(Dense(1, activation='sigmoid'))\n",
    "MIFilteredPLusTreeWrapperModel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "MIFilteredPLusTreeWrapperModel.fit(TopTreeXtrain, Y_train, validation_data=(TopTreeXval, Y_val), epochs=10, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = MIFilteredPLusTreeWrapperModel.predict_classes(TopTreeXtest)\n",
    "a = ConMatAndStats (Ytest, Ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"Full Run Time = %.3f\" % (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
